{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install glfw\n",
        "!pip install mujoco"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncy4kRkUYEEo",
        "outputId": "ad37f4ef-f4cc-4fb4-fa38-bb6f78d3adfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: glfw in /usr/local/lib/python3.11/dist-packages (2.8.0)\n",
            "Collecting mujoco\n",
            "  Downloading mujoco-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco) (1.12.2)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.11/dist-packages (from mujoco) (2.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mujoco) (2.0.2)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco) (4.13.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco) (3.21.0)\n",
            "Downloading mujoco-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mujoco\n",
            "Successfully installed mujoco-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "if not hasattr(np, 'bool8'):\n",
        "    np.bool8 = np.bool_"
      ],
      "metadata": {
        "id": "Fx9cUXjiX5Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------\n",
        "# 1. Define the DDPG components for continuous action spaces\n",
        "# ------------------------------\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, action_dim)\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.max_action * torch.tanh(self.fc3(x))\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        # Q1\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "        # Q2\n",
        "        self.fc4 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.fc5 = nn.Linear(256, 256)\n",
        "        self.fc6 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        sa = torch.cat([state, action], 1)\n",
        "\n",
        "        # Q1\n",
        "        q1 = torch.relu(self.fc1(sa))\n",
        "        q1 = torch.relu(self.fc2(q1))\n",
        "        q1 = self.fc3(q1)\n",
        "\n",
        "        # Q2\n",
        "        q2 = torch.relu(self.fc4(sa))\n",
        "        q2 = torch.relu(self.fc5(q2))\n",
        "        q2 = self.fc6(q2)\n",
        "\n",
        "        return q1, q2\n",
        "\n",
        "    def Q1(self, state, action):\n",
        "        sa = torch.cat([state, action], 1)\n",
        "        q1 = torch.relu(self.fc1(sa))\n",
        "        q1 = torch.relu(self.fc2(q1))\n",
        "        q1 = self.fc3(q1)\n",
        "        return q1\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = zip(*batch)\n",
        "        return np.array(state), np.array(action), reward, np.array(next_state), done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# -----------------------------------\n",
        "# 2. Define wrappers for uncertainties\n",
        "# -----------------------------------\n",
        "\n",
        "# 2.1 Sensor/Observation Noise: add Gaussian noise to observations\n",
        "class SensorNoiseWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, noise_std=0.01):\n",
        "        super(SensorNoiseWrapper, self).__init__(env)\n",
        "        self.noise_std = noise_std\n",
        "\n",
        "    def observation(self, observation):\n",
        "        noise = np.random.normal(0, self.noise_std, size=observation.shape)\n",
        "        return observation + noise\n",
        "\n",
        "# 2.2 Motor Noise: add random noise to actions (torque outputs)\n",
        "class MotorNoiseWrapper(gym.ActionWrapper):\n",
        "    def __init__(self, env, noise_std=0.05):\n",
        "        super(MotorNoiseWrapper, self).__init__(env)\n",
        "        self.noise_std = noise_std\n",
        "\n",
        "    def action(self, action):\n",
        "        noise = np.random.normal(0, self.noise_std, size=action.shape)\n",
        "        return np.clip(action + noise, self.action_space.low, self.action_space.high)\n",
        "\n",
        "# 2.3 Leg Mass or Joint Stiffness Variation:\n",
        "#     Modify mass properties or joint dynamics on reset.\n",
        "class MassVariabilityWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, mass_variation_range=(0.8, 1.2)):\n",
        "        super(MassVariabilityWrapper, self).__init__(env)\n",
        "        self.mass_variation_range = mass_variation_range\n",
        "        self.original_body_mass = None\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        observation = self.env.reset(**kwargs)\n",
        "\n",
        "        # Store original masses if not stored already\n",
        "        if self.original_body_mass is None and hasattr(self.env.unwrapped, 'model'):\n",
        "            self.original_body_mass = self.env.unwrapped.model.body_mass.copy()\n",
        "\n",
        "        # Apply random mass variations if model exists\n",
        "        if hasattr(self.env.unwrapped, 'model') and self.original_body_mass is not None:\n",
        "            # Focus on leg masses (depending on the specific model structure)\n",
        "            # Indices 4-8 typically correspond to the leg parts in HalfCheetah\n",
        "            leg_indices = range(4, 9)  # This may need adjustment based on the actual model\n",
        "\n",
        "            for idx in leg_indices:\n",
        "                if idx < len(self.original_body_mass):\n",
        "                    variation = np.random.uniform(*self.mass_variation_range)\n",
        "                    self.env.unwrapped.model.body_mass[idx] = self.original_body_mass[idx] * variation\n",
        "\n",
        "        return observation\n",
        "\n",
        "# 2.4 Random Drag or Terrain Resistance:\n",
        "#     Apply varying levels of drag to the cheetah's movement.\n",
        "class TerrainResistanceWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, drag_range=(0.0, 0.3)):\n",
        "        super(TerrainResistanceWrapper, self).__init__(env)\n",
        "        self.drag_range = drag_range\n",
        "        self.current_drag = 0.0\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        observation = self.env.reset(**kwargs)\n",
        "        # Set a new random drag coefficient for this episode\n",
        "        self.current_drag = np.random.uniform(*self.drag_range)\n",
        "        return observation\n",
        "\n",
        "    def step(self, action):\n",
        "        result = self.env.step(action)\n",
        "\n",
        "        # Handle both the new_step_api (5 values) and old (4 values) cases\n",
        "        if len(result) == 5:\n",
        "            obs, reward, done, truncated, info = result\n",
        "            done = done or truncated\n",
        "        else:\n",
        "            obs, reward, done, info = result\n",
        "\n",
        "        # Apply drag by modifying velocity components if we can access them\n",
        "        if hasattr(self.env.unwrapped, 'sim'):\n",
        "            # Get current velocities\n",
        "            qvel = self.env.unwrapped.sim.data.qvel.copy()\n",
        "\n",
        "            # Apply drag to horizontal velocity (typically the first velocity component)\n",
        "            if len(qvel) > 0:\n",
        "                qvel[0] *= (1.0 - self.current_drag)\n",
        "\n",
        "                # Update velocities in the simulation\n",
        "                self.env.unwrapped.sim.data.qvel[:] = qvel\n",
        "\n",
        "        if len(result) == 5:\n",
        "            return obs, reward, done, truncated, info\n",
        "        else:\n",
        "            return obs, reward, done, info\n",
        "\n",
        "# 2.5 External Force Pulses:\n",
        "#     Randomly apply external forces to simulate impacts.\n",
        "class ExternalForceWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, force_magnitude_range=(-100.0, 100.0), pulse_probability=0.05):\n",
        "        super(ExternalForceWrapper, self).__init__(env)\n",
        "        self.force_magnitude_range = force_magnitude_range\n",
        "        self.pulse_probability = pulse_probability\n",
        "\n",
        "    def step(self, action):\n",
        "        result = self.env.step(action)\n",
        "\n",
        "        # Handle both return formats\n",
        "        if len(result) == 5:\n",
        "            obs, reward, done, truncated, info = result\n",
        "            done = done or truncated\n",
        "        else:\n",
        "            obs, reward, done, info = result\n",
        "\n",
        "        # Randomly apply force pulses\n",
        "        if np.random.rand() < self.pulse_probability and hasattr(self.env.unwrapped, 'sim'):\n",
        "            # Choose a random force magnitude\n",
        "            force_magnitude = np.random.uniform(*self.force_magnitude_range)\n",
        "\n",
        "            # Apply the force to the torso (typically body index 1)\n",
        "            torso_idx = 1  # This may need adjustment based on the actual model\n",
        "            if hasattr(self.env.unwrapped.sim, 'data'):\n",
        "                # Create force vector [x, y, z] - mainly in x direction (forward/backward)\n",
        "                force = np.array([force_magnitude, 0.0, 0.0])\n",
        "\n",
        "                # Apply the force if the method exists\n",
        "                if hasattr(self.env.unwrapped.sim, 'apply_force'):\n",
        "                    self.env.unwrapped.sim.apply_force(force, torso_idx)\n",
        "\n",
        "        if len(result) == 5:\n",
        "            return obs, reward, done, truncated, info\n",
        "        else:\n",
        "            return obs, reward, done, info\n",
        "\n",
        "# 2.6 Variable Contact Softness:\n",
        "#     Change ground contact parameters to simulate different surfaces.\n",
        "class VariableContactWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, stiffness_range=(1.0, 10.0), damping_range=(0.1, 1.0)):\n",
        "        super(VariableContactWrapper, self).__init__(env)\n",
        "        self.stiffness_range = stiffness_range\n",
        "        self.damping_range = damping_range\n",
        "        self.original_stiffness = None\n",
        "        self.original_damping = None\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        observation = self.env.reset(**kwargs)\n",
        "\n",
        "        # Store original parameters if not stored already\n",
        "        if self.original_stiffness is None and hasattr(self.env.unwrapped, 'model'):\n",
        "            if hasattr(self.env.unwrapped.model, 'geom_kp'):\n",
        "                self.original_stiffness = self.env.unwrapped.model.geom_kp.copy()\n",
        "            if hasattr(self.env.unwrapped.model, 'geom_kd'):\n",
        "                self.original_damping = self.env.unwrapped.model.geom_kd.copy()\n",
        "\n",
        "        # Apply random contact parameters\n",
        "        if hasattr(self.env.unwrapped, 'model'):\n",
        "            # Adjust contact stiffness\n",
        "            if hasattr(self.env.unwrapped.model, 'geom_kp') and self.original_stiffness is not None:\n",
        "                stiffness_factor = np.random.uniform(*self.stiffness_range)\n",
        "                # Apply to ground contact geoms (usually the first few indices)\n",
        "                ground_indices = [0]  # Typically the first geom is the ground\n",
        "                for idx in ground_indices:\n",
        "                    if idx < len(self.original_stiffness):\n",
        "                        self.env.unwrapped.model.geom_kp[idx] = self.original_stiffness[idx] * stiffness_factor\n",
        "\n",
        "            # Adjust contact damping\n",
        "            if hasattr(self.env.unwrapped.model, 'geom_kd') and self.original_damping is not None:\n",
        "                damping_factor = np.random.uniform(*self.damping_range)\n",
        "                for idx in ground_indices:\n",
        "                    if idx < len(self.original_damping):\n",
        "                        self.env.unwrapped.model.geom_kd[idx] = self.original_damping[idx] * damping_factor\n",
        "\n",
        "        return observation\n",
        "\n",
        "# 2.7 Unified Aleatoric Disruption: applies random state perturbations based on λ and I\n",
        "class DisruptionWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, lambda_rate=0.1, intensity_scale=0.05):\n",
        "        super(DisruptionWrapper, self).__init__(env)\n",
        "        self.lambda_rate = lambda_rate\n",
        "        # Calculate intensity vector proportional to observation space shape\n",
        "        state_dim = env.observation_space.shape[0]\n",
        "        self.intensity_vector = np.ones(state_dim) * intensity_scale\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        result = self.env.step(action)\n",
        "        if len(result) == 5:\n",
        "            obs, reward, done, truncated, info = result\n",
        "            done = done or truncated\n",
        "        else:\n",
        "            obs, reward, done, info = result\n",
        "\n",
        "        # Apply disruption with probability λ\n",
        "        if np.random.rand() < self.lambda_rate:\n",
        "            noise = np.random.uniform(-self.intensity_vector, self.intensity_vector)\n",
        "            obs = obs + noise\n",
        "\n",
        "            # Optional: update underlying simulation state if possible\n",
        "            if hasattr(self.env.unwrapped, 'sim') and hasattr(self.env.unwrapped.sim, 'data'):\n",
        "                # This is more complex for MuJoCo and depends on how the state is represented\n",
        "                pass\n",
        "\n",
        "        if len(result) == 5:\n",
        "            return obs, reward, done, truncated, info\n",
        "        else:\n",
        "            return obs, reward, done, info\n",
        "\n",
        "# -----------------------------------------\n",
        "# 3. Training function for the DDPG agent\n",
        "# -----------------------------------------\n",
        "\n",
        "def train_ddpg(env_fn, episodes=200, batch_size=64, gamma=0.99, lr=3e-4,\n",
        "              tau=0.005, exploration_noise=0.1, memory_capacity=100000):\n",
        "    env = env_fn()\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "\n",
        "    # Initialize actor and critic networks\n",
        "    actor = Actor(state_dim, action_dim, max_action)\n",
        "    actor_target = Actor(state_dim, action_dim, max_action)\n",
        "    actor_target.load_state_dict(actor.state_dict())\n",
        "\n",
        "    critic = Critic(state_dim, action_dim)\n",
        "    critic_target = Critic(state_dim, action_dim)\n",
        "    critic_target.load_state_dict(critic.state_dict())\n",
        "\n",
        "    # Initialize optimizers\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=lr)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=lr)\n",
        "\n",
        "    # Initialize replay buffer\n",
        "    memory = ReplayBuffer(memory_capacity)\n",
        "\n",
        "    rewards_history = []\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        if isinstance(state, tuple):\n",
        "            state = state[0]  # Handle new env reset API\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Select action with exploration noise\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                action = actor(state_tensor).squeeze().numpy()\n",
        "            action = action + np.random.normal(0, exploration_noise, size=action_dim)\n",
        "            action = np.clip(action, -max_action, max_action)\n",
        "\n",
        "            # Take action in environment\n",
        "            step_result = env.step(action)\n",
        "            if len(step_result) == 5:\n",
        "                next_state, reward, done, truncated, _ = step_result\n",
        "                done = done or truncated\n",
        "            else:\n",
        "                next_state, reward, done, _ = step_result\n",
        "\n",
        "            # Store transition in replay buffer\n",
        "            memory.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            # Update networks if enough samples\n",
        "            if len(memory) > batch_size:\n",
        "                states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
        "\n",
        "                # Convert to tensors\n",
        "                states = torch.FloatTensor(states)\n",
        "                actions = torch.FloatTensor(actions)\n",
        "                rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
        "                next_states = torch.FloatTensor(next_states)\n",
        "                dones = torch.FloatTensor([float(d) for d in dones]).unsqueeze(1)\n",
        "\n",
        "                # Update critic\n",
        "                with torch.no_grad():\n",
        "                    next_actions = actor_target(next_states)\n",
        "                    target_q1, target_q2 = critic_target(next_states, next_actions)\n",
        "                    target_q = torch.min(target_q1, target_q2)\n",
        "                    target_q = rewards + (1 - dones) * gamma * target_q\n",
        "\n",
        "                current_q1, current_q2 = critic(states, actions)\n",
        "                critic_loss = nn.MSELoss()(current_q1, target_q) + nn.MSELoss()(current_q2, target_q)\n",
        "\n",
        "                critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                critic_optimizer.step()\n",
        "\n",
        "                # Update actor (less frequently)\n",
        "                if episode % 2 == 0:\n",
        "                    actor_loss = -critic.Q1(states, actor(states)).mean()\n",
        "\n",
        "                    actor_optimizer.zero_grad()\n",
        "                    actor_loss.backward()\n",
        "                    actor_optimizer.step()\n",
        "\n",
        "                    # Soft update target networks\n",
        "                    for param, target_param in zip(critic.parameters(), critic_target.parameters()):\n",
        "                        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "                    for param, target_param in zip(actor.parameters(), actor_target.parameters()):\n",
        "                        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        rewards_history.append(total_reward)\n",
        "        print(f\"Episode {episode}, Reward: {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    return rewards_history\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# 4. Create environment functions for each uncertainty\n",
        "# ------------------------------------------------------\n",
        "\n",
        "def make_base_env():\n",
        "    # Base environment (no uncertainty)\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    return env\n",
        "\n",
        "def make_sensor_noise_env():\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    env = SensorNoiseWrapper(env, noise_std=0.01)\n",
        "    return env\n",
        "\n",
        "def make_motor_noise_env():\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    env = MotorNoiseWrapper(env, noise_std=0.05)\n",
        "    return env\n",
        "\n",
        "def make_mass_variability_env():\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    env = MassVariabilityWrapper(env, mass_variation_range=(0.8, 1.2))\n",
        "    return env\n",
        "\n",
        "def make_terrain_resistance_env():\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    env = TerrainResistanceWrapper(env, drag_range=(0.0, 0.3))\n",
        "    return env\n",
        "\n",
        "def make_external_force_env():\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    env = ExternalForceWrapper(env, force_magnitude_range=(-100.0, 100.0), pulse_probability=0.05)\n",
        "    return env\n",
        "\n",
        "def make_variable_contact_env():\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    env = VariableContactWrapper(env, stiffness_range=(1.0, 10.0), damping_range=(0.1, 1.0))\n",
        "    return env\n",
        "\n",
        "def make_disruption_env():\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    env = DisruptionWrapper(env, lambda_rate=0.1, intensity_scale=0.05)\n",
        "    return env\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5. Run experiments and collect performance metrics\n",
        "# -------------------------------------------------\n",
        "\n",
        "experiments = {\n",
        "    \"Base\": make_base_env,\n",
        "    \"Sensor Noise\": make_sensor_noise_env,\n",
        "    \"Motor Noise\": make_motor_noise_env,\n",
        "    \"Mass Variability\": make_mass_variability_env,\n",
        "    \"Terrain Resistance\": make_terrain_resistance_env,\n",
        "    \"External Force\": make_external_force_env,\n",
        "    \"Variable Contact\": make_variable_contact_env,\n",
        "    \"Disruption Model\": make_disruption_env\n",
        "}\n",
        "\n",
        "results = {}\n",
        "episodes = 300  # Adjust as needed (HalfCheetah may need more episodes)\n",
        "\n",
        "for key, env_fn in experiments.items():\n",
        "    print(f\"\\nTraining with {key} uncertainty:\")\n",
        "    rewards = train_ddpg(env_fn, episodes=episodes)\n",
        "    results[key] = rewards\n",
        "\n",
        "# --------------------------------------\n",
        "# 6. Plot the training performance curves\n",
        "# --------------------------------------\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for key, rewards in results.items():\n",
        "    # Apply smoothing for clearer visualization\n",
        "    window_size = 10\n",
        "    smoothed_rewards = [np.mean(rewards[max(0, i-window_size):i+1]) for i in range(len(rewards))]\n",
        "    plt.plot(smoothed_rewards, label=key)\n",
        "\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"RL Agent Performance under Different Aleatoric Uncertainties in HalfCheetah\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DGvXugXhYe7N",
        "outputId": "1a79820a-bbff-4560-eba8-0a36792f83c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training with Base uncertainty:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Reward: -349.54\n",
            "Episode 1, Reward: 610.68\n",
            "Episode 2, Reward: -331.78\n",
            "Episode 3, Reward: -671.25\n",
            "Episode 4, Reward: 59.39\n",
            "Episode 5, Reward: 302.86\n",
            "Episode 6, Reward: 347.81\n",
            "Episode 7, Reward: 452.61\n",
            "Episode 8, Reward: 408.71\n",
            "Episode 9, Reward: 419.97\n",
            "Episode 10, Reward: 558.38\n",
            "Episode 11, Reward: 1336.77\n",
            "Episode 12, Reward: 1283.36\n",
            "Episode 13, Reward: 1223.43\n",
            "Episode 14, Reward: 1421.08\n",
            "Episode 15, Reward: 1543.95\n",
            "Episode 16, Reward: 1567.50\n",
            "Episode 17, Reward: 1727.43\n",
            "Episode 18, Reward: 1155.98\n",
            "Episode 19, Reward: 1224.95\n",
            "Episode 20, Reward: 1218.15\n",
            "Episode 21, Reward: 758.54\n",
            "Episode 22, Reward: 2072.45\n",
            "Episode 23, Reward: 1440.37\n",
            "Episode 24, Reward: 2109.81\n",
            "Episode 25, Reward: 2220.23\n",
            "Episode 26, Reward: 1748.39\n",
            "Episode 27, Reward: 2053.00\n",
            "Episode 28, Reward: 331.14\n",
            "Episode 29, Reward: 907.83\n",
            "Episode 30, Reward: 770.35\n",
            "Episode 31, Reward: 2202.58\n",
            "Episode 32, Reward: 2271.79\n",
            "Episode 33, Reward: 2489.98\n",
            "Episode 34, Reward: 2491.38\n",
            "Episode 35, Reward: 2263.14\n",
            "Episode 36, Reward: 2529.56\n",
            "Episode 37, Reward: 2213.51\n",
            "Episode 38, Reward: 2382.73\n",
            "Episode 39, Reward: 2575.09\n",
            "Episode 40, Reward: 2382.58\n",
            "Episode 41, Reward: 2445.76\n",
            "Episode 42, Reward: 2495.63\n",
            "Episode 43, Reward: 2283.33\n",
            "Episode 44, Reward: 2764.29\n",
            "Episode 45, Reward: 2526.20\n",
            "Episode 46, Reward: 2653.03\n",
            "Episode 47, Reward: 2418.08\n",
            "Episode 48, Reward: 2328.50\n",
            "Episode 49, Reward: 2654.71\n",
            "Episode 50, Reward: 2659.79\n",
            "Episode 51, Reward: 2452.30\n",
            "Episode 52, Reward: 2638.07\n",
            "Episode 53, Reward: 2303.60\n",
            "Episode 54, Reward: 2525.13\n",
            "Episode 55, Reward: 2717.88\n",
            "Episode 56, Reward: 2588.34\n",
            "Episode 57, Reward: 2227.94\n",
            "Episode 58, Reward: 2579.94\n",
            "Episode 59, Reward: 2596.93\n",
            "Episode 60, Reward: 2730.18\n",
            "Episode 61, Reward: 2236.02\n",
            "Episode 62, Reward: 2721.21\n",
            "Episode 63, Reward: 2566.27\n",
            "Episode 64, Reward: 2400.11\n",
            "Episode 65, Reward: 2520.30\n",
            "Episode 66, Reward: 2627.11\n",
            "Episode 67, Reward: 2242.65\n",
            "Episode 68, Reward: 2699.75\n",
            "Episode 69, Reward: 2434.40\n",
            "Episode 70, Reward: 2568.14\n",
            "Episode 71, Reward: 2713.28\n",
            "Episode 72, Reward: 2601.65\n",
            "Episode 73, Reward: 2501.51\n",
            "Episode 74, Reward: 2704.47\n",
            "Episode 75, Reward: 2550.20\n",
            "Episode 76, Reward: 2690.62\n",
            "Episode 77, Reward: 2690.91\n",
            "Episode 78, Reward: 2364.86\n",
            "Episode 79, Reward: 2667.92\n",
            "Episode 80, Reward: 2493.81\n",
            "Episode 81, Reward: -685.18\n",
            "Episode 82, Reward: 2570.08\n",
            "Episode 83, Reward: 2655.88\n",
            "Episode 84, Reward: 2663.36\n",
            "Episode 85, Reward: 2562.22\n",
            "Episode 86, Reward: 2685.11\n",
            "Episode 87, Reward: 2782.50\n",
            "Episode 88, Reward: 2774.07\n",
            "Episode 89, Reward: 2641.50\n",
            "Episode 90, Reward: 2614.42\n",
            "Episode 91, Reward: 2402.65\n",
            "Episode 92, Reward: 2533.72\n",
            "Episode 93, Reward: 2746.76\n",
            "Episode 94, Reward: 2586.95\n",
            "Episode 95, Reward: 2603.36\n",
            "Episode 96, Reward: 2607.01\n",
            "Episode 97, Reward: 2716.16\n",
            "Episode 98, Reward: 2725.80\n",
            "Episode 99, Reward: 2509.25\n",
            "Episode 100, Reward: 2730.35\n",
            "Episode 101, Reward: 2702.08\n",
            "Episode 102, Reward: 2655.42\n",
            "Episode 103, Reward: 2540.50\n",
            "Episode 104, Reward: 2745.92\n",
            "Episode 105, Reward: 2769.83\n",
            "Episode 106, Reward: 2644.99\n",
            "Episode 107, Reward: 2596.56\n",
            "Episode 108, Reward: 2849.13\n",
            "Episode 109, Reward: 2733.23\n",
            "Episode 110, Reward: 2841.64\n",
            "Episode 111, Reward: 2553.96\n",
            "Episode 112, Reward: 2698.28\n",
            "Episode 113, Reward: 2702.59\n",
            "Episode 114, Reward: 2598.13\n",
            "Episode 115, Reward: 2673.04\n",
            "Episode 116, Reward: 2501.16\n",
            "Episode 117, Reward: 2712.79\n",
            "Episode 118, Reward: 2702.25\n",
            "Episode 119, Reward: 2746.83\n",
            "Episode 120, Reward: 2713.36\n",
            "Episode 121, Reward: 2732.15\n",
            "Episode 122, Reward: 2633.84\n",
            "Episode 123, Reward: 2534.95\n",
            "Episode 124, Reward: 2747.42\n",
            "Episode 125, Reward: 2708.91\n",
            "Episode 126, Reward: 2801.20\n",
            "Episode 127, Reward: 2752.83\n",
            "Episode 128, Reward: 2841.25\n",
            "Episode 129, Reward: 2773.15\n",
            "Episode 130, Reward: 2658.23\n",
            "Episode 131, Reward: 2597.18\n",
            "Episode 132, Reward: 2664.63\n",
            "Episode 133, Reward: 2772.80\n",
            "Episode 134, Reward: 2716.98\n",
            "Episode 135, Reward: 2734.21\n",
            "Episode 136, Reward: 2786.14\n",
            "Episode 137, Reward: 2858.82\n",
            "Episode 138, Reward: 339.05\n",
            "Episode 139, Reward: 2794.34\n",
            "Episode 140, Reward: 2785.91\n",
            "Episode 141, Reward: 2746.77\n",
            "Episode 142, Reward: 2842.81\n",
            "Episode 143, Reward: 2624.41\n",
            "Episode 144, Reward: 2758.70\n",
            "Episode 145, Reward: 2659.96\n",
            "Episode 146, Reward: 2916.92\n",
            "Episode 147, Reward: 2805.07\n",
            "Episode 148, Reward: 2780.30\n",
            "Episode 149, Reward: 2795.39\n",
            "Episode 150, Reward: 2851.26\n",
            "Episode 151, Reward: 2519.60\n",
            "Episode 152, Reward: 2812.75\n",
            "Episode 153, Reward: 2595.99\n",
            "Episode 154, Reward: 2595.07\n",
            "Episode 155, Reward: 2640.74\n",
            "Episode 156, Reward: 2831.03\n",
            "Episode 157, Reward: 2511.64\n",
            "Episode 158, Reward: 2557.63\n",
            "Episode 159, Reward: 2684.41\n",
            "Episode 160, Reward: 2781.98\n",
            "Episode 161, Reward: 2735.25\n",
            "Episode 162, Reward: 2768.60\n",
            "Episode 163, Reward: 2613.19\n",
            "Episode 164, Reward: 2629.78\n",
            "Episode 165, Reward: 2617.14\n",
            "Episode 166, Reward: 2722.87\n",
            "Episode 167, Reward: 2814.71\n",
            "Episode 168, Reward: 2758.56\n",
            "Episode 169, Reward: 2721.18\n",
            "Episode 170, Reward: 2857.78\n",
            "Episode 171, Reward: 2860.38\n",
            "Episode 172, Reward: 2670.66\n",
            "Episode 173, Reward: 2730.60\n",
            "Episode 174, Reward: 2792.12\n",
            "Episode 175, Reward: 2671.05\n",
            "Episode 176, Reward: 2645.48\n",
            "Episode 177, Reward: 2900.48\n",
            "Episode 178, Reward: 2740.07\n",
            "Episode 179, Reward: 2575.12\n",
            "Episode 180, Reward: 2828.19\n",
            "Episode 181, Reward: 2844.03\n",
            "Episode 182, Reward: 2665.86\n",
            "Episode 183, Reward: 2946.38\n",
            "Episode 184, Reward: 2974.31\n",
            "Episode 185, Reward: 2572.97\n",
            "Episode 186, Reward: 2671.08\n",
            "Episode 187, Reward: 2742.65\n",
            "Episode 188, Reward: 2604.71\n",
            "Episode 189, Reward: 2643.13\n",
            "Episode 190, Reward: 2671.23\n",
            "Episode 191, Reward: 2578.88\n",
            "Episode 192, Reward: 2623.51\n",
            "Episode 193, Reward: 2409.91\n",
            "Episode 194, Reward: 2763.61\n",
            "Episode 195, Reward: 2666.46\n",
            "Episode 196, Reward: 2617.02\n",
            "Episode 197, Reward: 2571.20\n",
            "Episode 198, Reward: 2641.49\n",
            "Episode 199, Reward: 2669.20\n",
            "Episode 200, Reward: 2859.10\n",
            "Episode 201, Reward: 2645.80\n",
            "Episode 202, Reward: 2505.87\n",
            "Episode 203, Reward: 2660.45\n",
            "Episode 204, Reward: 2773.72\n",
            "Episode 205, Reward: 2830.12\n",
            "Episode 206, Reward: 2884.58\n",
            "Episode 207, Reward: 2816.68\n",
            "Episode 208, Reward: 2724.31\n",
            "Episode 209, Reward: 2650.96\n",
            "Episode 210, Reward: 2439.91\n",
            "Episode 211, Reward: 2485.64\n",
            "Episode 212, Reward: 838.67\n",
            "Episode 213, Reward: 2589.19\n",
            "Episode 214, Reward: 2501.84\n",
            "Episode 215, Reward: 2776.90\n",
            "Episode 216, Reward: 2587.04\n",
            "Episode 217, Reward: 2755.24\n",
            "Episode 218, Reward: 2614.15\n",
            "Episode 219, Reward: 2744.87\n",
            "Episode 220, Reward: 2763.22\n",
            "Episode 221, Reward: 2892.32\n",
            "Episode 222, Reward: 2659.34\n",
            "Episode 223, Reward: 2452.68\n",
            "Episode 224, Reward: 2486.45\n",
            "Episode 225, Reward: 2536.95\n",
            "Episode 226, Reward: 2706.15\n",
            "Episode 227, Reward: 2618.88\n",
            "Episode 228, Reward: 2735.48\n",
            "Episode 229, Reward: 2857.52\n",
            "Episode 230, Reward: 1062.08\n",
            "Episode 231, Reward: 2692.56\n",
            "Episode 232, Reward: 2638.25\n",
            "Episode 233, Reward: 2654.04\n",
            "Episode 234, Reward: 2608.26\n",
            "Episode 235, Reward: 2795.62\n",
            "Episode 236, Reward: 2578.53\n",
            "Episode 237, Reward: 2884.88\n",
            "Episode 238, Reward: 2793.06\n",
            "Episode 239, Reward: 2757.25\n",
            "Episode 240, Reward: 2671.72\n",
            "Episode 241, Reward: 3011.60\n",
            "Episode 242, Reward: 2903.21\n",
            "Episode 243, Reward: 2660.67\n",
            "Episode 244, Reward: 2979.14\n",
            "Episode 245, Reward: 2957.51\n",
            "Episode 246, Reward: 2951.92\n",
            "Episode 247, Reward: 2886.10\n",
            "Episode 248, Reward: 2756.42\n",
            "Episode 249, Reward: 2905.42\n",
            "Episode 250, Reward: 2601.66\n",
            "Episode 251, Reward: 2668.13\n",
            "Episode 252, Reward: 2511.09\n",
            "Episode 253, Reward: 2820.96\n",
            "Episode 254, Reward: 2520.04\n",
            "Episode 255, Reward: 126.18\n",
            "Episode 256, Reward: 2855.18\n",
            "Episode 257, Reward: 2693.76\n",
            "Episode 258, Reward: 2899.65\n",
            "Episode 259, Reward: 2037.03\n",
            "Episode 260, Reward: 2698.26\n",
            "Episode 261, Reward: 2843.25\n",
            "Episode 262, Reward: 2605.96\n",
            "Episode 263, Reward: 2978.34\n",
            "Episode 264, Reward: 2912.79\n",
            "Episode 265, Reward: 2815.46\n",
            "Episode 266, Reward: 2728.47\n",
            "Episode 267, Reward: 3045.44\n",
            "Episode 268, Reward: 2784.11\n",
            "Episode 269, Reward: 2889.61\n",
            "Episode 270, Reward: 2742.90\n",
            "Episode 271, Reward: 2911.65\n",
            "Episode 272, Reward: 2756.95\n",
            "Episode 273, Reward: 2792.40\n",
            "Episode 274, Reward: 2924.86\n",
            "Episode 275, Reward: 2627.58\n",
            "Episode 276, Reward: 2837.98\n",
            "Episode 277, Reward: 2867.11\n",
            "Episode 278, Reward: 2788.20\n",
            "Episode 279, Reward: 2784.38\n",
            "Episode 280, Reward: 2784.12\n",
            "Episode 281, Reward: 2716.35\n",
            "Episode 282, Reward: 2813.05\n",
            "Episode 283, Reward: 2854.51\n",
            "Episode 284, Reward: 2945.81\n",
            "Episode 285, Reward: 2953.67\n",
            "Episode 286, Reward: 2814.18\n",
            "Episode 287, Reward: 2862.48\n",
            "Episode 288, Reward: 2649.53\n",
            "Episode 289, Reward: 2891.76\n",
            "Episode 290, Reward: 2934.49\n",
            "Episode 291, Reward: 2952.99\n",
            "Episode 292, Reward: 2770.01\n",
            "Episode 293, Reward: 2963.85\n",
            "Episode 294, Reward: 2966.65\n",
            "Episode 295, Reward: 2820.24\n",
            "Episode 296, Reward: 2813.57\n",
            "Episode 297, Reward: 2803.12\n",
            "Episode 298, Reward: 3072.19\n",
            "Episode 299, Reward: 2884.61\n",
            "\n",
            "Training with Sensor Noise uncertainty:\n",
            "Episode 0, Reward: -393.43\n",
            "Episode 1, Reward: -446.31\n",
            "Episode 2, Reward: -589.30\n",
            "Episode 3, Reward: -101.70\n",
            "Episode 4, Reward: -259.58\n",
            "Episode 5, Reward: 185.15\n",
            "Episode 6, Reward: 138.61\n",
            "Episode 7, Reward: 2.48\n",
            "Episode 8, Reward: -209.09\n",
            "Episode 9, Reward: -146.19\n",
            "Episode 10, Reward: -20.62\n",
            "Episode 11, Reward: 450.90\n",
            "Episode 12, Reward: 785.36\n",
            "Episode 13, Reward: 995.44\n",
            "Episode 14, Reward: 869.83\n",
            "Episode 15, Reward: 1020.93\n",
            "Episode 16, Reward: 1319.03\n",
            "Episode 17, Reward: 1043.58\n",
            "Episode 18, Reward: 1499.20\n",
            "Episode 19, Reward: 1503.51\n",
            "Episode 20, Reward: 1347.80\n",
            "Episode 21, Reward: 393.79\n",
            "Episode 22, Reward: 1424.22\n",
            "Episode 23, Reward: 1495.48\n",
            "Episode 24, Reward: -332.93\n",
            "Episode 25, Reward: 1577.82\n",
            "Episode 26, Reward: 1590.90\n",
            "Episode 27, Reward: -353.29\n",
            "Episode 28, Reward: 437.04\n",
            "Episode 29, Reward: 1492.61\n",
            "Episode 30, Reward: 1802.96\n",
            "Episode 31, Reward: 1582.24\n",
            "Episode 32, Reward: 1768.67\n",
            "Episode 33, Reward: 1682.68\n",
            "Episode 34, Reward: 1918.84\n",
            "Episode 35, Reward: -149.06\n",
            "Episode 36, Reward: 490.87\n",
            "Episode 37, Reward: 1406.84\n",
            "Episode 38, Reward: 657.10\n",
            "Episode 39, Reward: 1905.07\n",
            "Episode 40, Reward: 1786.60\n",
            "Episode 41, Reward: 1398.70\n",
            "Episode 42, Reward: 1919.08\n",
            "Episode 43, Reward: 1794.19\n",
            "Episode 44, Reward: 1640.00\n",
            "Episode 45, Reward: 2075.11\n",
            "Episode 46, Reward: 2184.80\n",
            "Episode 47, Reward: -285.99\n",
            "Episode 48, Reward: 2076.08\n",
            "Episode 49, Reward: 476.58\n",
            "Episode 50, Reward: 47.24\n",
            "Episode 51, Reward: 120.34\n",
            "Episode 52, Reward: 1919.27\n",
            "Episode 53, Reward: 1345.67\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6f30391c64b8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_fn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTraining with {key} uncertainty:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6f30391c64b8>\u001b[0m in \u001b[0;36mtrain_ddpg\u001b[0;34m(env_fn, episodes, batch_size, gamma, lr, tau, exploration_noise, memory_capacity)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m                 \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m                 \u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------\n",
        "# 1. GPU Configuration\n",
        "# ------------------------------\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ------------------------------\n",
        "# 2. Define the PPO components for continuous action spaces\n",
        "# ------------------------------\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        # Shared network layers\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(state_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Actor (policy) network\n",
        "        self.mean = nn.Linear(256, action_dim)\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "\n",
        "        # # Critic (value) network\n",
        "        # self.critic = nn.Linear(256, 1)\n",
        "\n",
        "        # Critic (value) network - rename to value_layer\n",
        "        self.value_layer = nn.Linear(256, 1)  # Renamed from \"critic\"\n",
        "\n",
        "        # Action scale\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self):\n",
        "        # Not used directly\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def actor(self, state):\n",
        "        x = self.shared(state)\n",
        "        mean = self.mean(x)\n",
        "        log_std = self.log_std.expand_as(mean)\n",
        "        # Clamp log_std to prevent very small or large values\n",
        "        log_std = torch.clamp(log_std, -20, 2)\n",
        "        std = torch.exp(log_std)\n",
        "\n",
        "        return mean, std\n",
        "\n",
        "    def critic(self, state):\n",
        "        x = self.shared(state)\n",
        "        value = self.value_layer(x)  # Use the renamed layer\n",
        "        #value = self.critic(x)\n",
        "        return value\n",
        "\n",
        "    def get_action(self, state, deterministic=False):\n",
        "        mean, std = self.actor(state)\n",
        "\n",
        "        if deterministic:\n",
        "            return self.max_action * torch.tanh(mean)\n",
        "\n",
        "        dist = Normal(mean, std)\n",
        "        action = dist.sample()\n",
        "        # Tanh squashing for bounded actions\n",
        "        action = self.max_action * torch.tanh(action)\n",
        "\n",
        "        # Calculate log probability for the sampled action\n",
        "        # We need to account for the tanh transformation when computing log probs\n",
        "        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)\n",
        "\n",
        "        # Apply tanh to keep actions within bounds\n",
        "        return action, log_prob\n",
        "\n",
        "    def evaluate_actions(self, state, action):\n",
        "        mean, std = self.actor(state)\n",
        "        dist = Normal(mean, std)\n",
        "\n",
        "        # Get log probability\n",
        "        log_probs = dist.log_prob(action).sum(dim=-1, keepdim=True)\n",
        "\n",
        "        # Get entropy for exploration\n",
        "        entropy = dist.entropy().mean()\n",
        "\n",
        "        # Get state value\n",
        "        value = self.critic(state)\n",
        "\n",
        "        return log_probs, entropy, value\n",
        "\n",
        "# Simple buffer for PPO\n",
        "class PPOBuffer:\n",
        "    def __init__(self, capacity, state_dim, action_dim):\n",
        "        self.states = np.zeros((capacity, state_dim), dtype=np.float32)\n",
        "        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)\n",
        "        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n",
        "        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)\n",
        "        self.dones = np.zeros((capacity, 1), dtype=np.float32)\n",
        "        self.log_probs = np.zeros((capacity, 1), dtype=np.float32)\n",
        "        self.values = np.zeros((capacity, 1), dtype=np.float32)\n",
        "\n",
        "        self.idx = 0\n",
        "        self.size = 0\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def store(self, state, action, reward, next_state, done, log_prob, value):\n",
        "        self.states[self.idx] = state\n",
        "        self.actions[self.idx] = action\n",
        "        self.rewards[self.idx] = reward\n",
        "        self.next_states[self.idx] = next_state\n",
        "        self.dones[self.idx] = done\n",
        "        self.log_probs[self.idx] = log_prob\n",
        "        self.values[self.idx] = value\n",
        "\n",
        "        self.idx = (self.idx + 1) % self.capacity\n",
        "        self.size = min(self.size + 1, self.capacity)\n",
        "\n",
        "    def get_all_data(self):\n",
        "        return (\n",
        "            self.states[:self.size],\n",
        "            self.actions[:self.size],\n",
        "            self.rewards[:self.size],\n",
        "            self.next_states[:self.size],\n",
        "            self.dones[:self.size],\n",
        "            self.log_probs[:self.size],\n",
        "            self.values[:self.size]\n",
        "        )\n",
        "\n",
        "    def clear(self):\n",
        "        self.idx = 0\n",
        "        self.size = 0\n",
        "\n",
        "# -----------------------------------\n",
        "# 3. Define wrappers for uncertainties (unchanged)\n",
        "# -----------------------------------\n",
        "\n",
        "# 3.1 Sensor/Observation Noise: add Gaussian noise to observations\n",
        "class SensorNoiseWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, noise_std=0.01):\n",
        "        super(SensorNoiseWrapper, self).__init__(env)\n",
        "        self.noise_std = noise_std\n",
        "\n",
        "    def observation(self, observation):\n",
        "        noise = np.random.normal(0, self.noise_std, size=observation.shape)\n",
        "        return observation + noise\n",
        "\n",
        "# 3.2 Motor Noise: add random noise to actions (torque outputs)\n",
        "class MotorNoiseWrapper(gym.ActionWrapper):\n",
        "    def __init__(self, env, noise_std=0.05):\n",
        "        super(MotorNoiseWrapper, self).__init__(env)\n",
        "        self.noise_std = noise_std\n",
        "\n",
        "    def action(self, action):\n",
        "        noise = np.random.normal(0, self.noise_std, size=action.shape)\n",
        "        return np.clip(action + noise, self.action_space.low, self.action_space.high)\n",
        "\n",
        "# 3.3 Leg Mass or Joint Stiffness Variation\n",
        "class MassVariabilityWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, mass_variation_range=(0.8, 1.2)):\n",
        "        super(MassVariabilityWrapper, self).__init__(env)\n",
        "        self.mass_variation_range = mass_variation_range\n",
        "        self.original_body_mass = None\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        observation = self.env.reset(**kwargs)\n",
        "        if isinstance(observation, tuple):\n",
        "            observation = observation[0]  # Handle new env reset API\n",
        "\n",
        "        # Store original masses if not stored already\n",
        "        if self.original_body_mass is None and hasattr(self.env.unwrapped, 'model'):\n",
        "            self.original_body_mass = self.env.unwrapped.model.body_mass.copy()\n",
        "\n",
        "        # Apply random mass variations if model exists\n",
        "        if hasattr(self.env.unwrapped, 'model') and self.original_body_mass is not None:\n",
        "            # Focus on leg masses (depending on the specific model structure)\n",
        "            # Indices 4-8 typically correspond to the leg parts in HalfCheetah\n",
        "            leg_indices = range(4, 9)  # Adjust based on actual model\n",
        "\n",
        "            for idx in leg_indices:\n",
        "                if idx < len(self.original_body_mass):\n",
        "                    variation = np.random.uniform(*self.mass_variation_range)\n",
        "                    self.env.unwrapped.model.body_mass[idx] = self.original_body_mass[idx] * variation\n",
        "\n",
        "        return observation\n",
        "\n",
        "# 3.4 Random Drag or Terrain Resistance\n",
        "class TerrainResistanceWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, drag_range=(0.0, 0.3)):\n",
        "        super(TerrainResistanceWrapper, self).__init__(env)\n",
        "        self.drag_range = drag_range\n",
        "        self.current_drag = 0.0\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        observation = self.env.reset(**kwargs)\n",
        "        if isinstance(observation, tuple):\n",
        "            observation = observation[0]  # Handle new env reset API\n",
        "\n",
        "        # Set a new random drag coefficient for this episode\n",
        "        self.current_drag = np.random.uniform(*self.drag_range)\n",
        "        return observation\n",
        "\n",
        "    def step(self, action):\n",
        "        result = self.env.step(action)\n",
        "\n",
        "        # Handle both the new_step_api (5 values) and old (4 values) cases\n",
        "        if len(result) == 5:\n",
        "            obs, reward, done, truncated, info = result\n",
        "            done = done or truncated\n",
        "        else:\n",
        "            obs, reward, done, info = result\n",
        "\n",
        "        # Apply drag by modifying velocity components if we can access them\n",
        "        if hasattr(self.env.unwrapped, 'sim'):\n",
        "            # Get current velocities\n",
        "            qvel = self.env.unwrapped.sim.data.qvel.copy()\n",
        "\n",
        "            # Apply drag to horizontal velocity (typically the first velocity component)\n",
        "            if len(qvel) > 0:\n",
        "                qvel[0] *= (1.0 - self.current_drag)\n",
        "\n",
        "                # Update velocities in the simulation\n",
        "                self.env.unwrapped.sim.data.qvel[:] = qvel\n",
        "\n",
        "        if len(result) == 5:\n",
        "            return obs, reward, done, truncated, info\n",
        "        else:\n",
        "            return obs, reward, done, info\n",
        "\n",
        "# 3.5 External Force Pulses\n",
        "class ExternalForceWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, force_magnitude_range=(-100.0, 100.0), pulse_probability=0.05):\n",
        "        super(ExternalForceWrapper, self).__init__(env)\n",
        "        self.force_magnitude_range = force_magnitude_range\n",
        "        self.pulse_probability = pulse_probability\n",
        "\n",
        "    def step(self, action):\n",
        "        result = self.env.step(action)\n",
        "\n",
        "        # Handle both return formats\n",
        "        if len(result) == 5:\n",
        "            obs, reward, done, truncated, info = result\n",
        "            done = done or truncated\n",
        "        else:\n",
        "            obs, reward, done, info = result\n",
        "\n",
        "        # Randomly apply force pulses\n",
        "        if np.random.rand() < self.pulse_probability and hasattr(self.env.unwrapped, 'sim'):\n",
        "            # Choose a random force magnitude\n",
        "            force_magnitude = np.random.uniform(*self.force_magnitude_range)\n",
        "\n",
        "            # Apply the force to the torso (typically body index 1)\n",
        "            torso_idx = 1  # Adjust based on actual model\n",
        "            if hasattr(self.env.unwrapped.sim, 'data'):\n",
        "                # Create force vector [x, y, z] - mainly in x direction (forward/backward)\n",
        "                force = np.array([force_magnitude, 0.0, 0.0])\n",
        "\n",
        "                # Apply the force if the method exists\n",
        "                if hasattr(self.env.unwrapped.sim, 'apply_force'):\n",
        "                    self.env.unwrapped.sim.apply_force(force, torso_idx)\n",
        "\n",
        "        if len(result) == 5:\n",
        "            return obs, reward, done, truncated, info\n",
        "        else:\n",
        "            return obs, reward, done, info\n",
        "\n",
        "# 3.6 Variable Contact Softness\n",
        "class VariableContactWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, stiffness_range=(1.0, 10.0), damping_range=(0.1, 1.0)):\n",
        "        super(VariableContactWrapper, self).__init__(env)\n",
        "        self.stiffness_range = stiffness_range\n",
        "        self.damping_range = damping_range\n",
        "        self.original_stiffness = None\n",
        "        self.original_damping = None\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        observation = self.env.reset(**kwargs)\n",
        "        if isinstance(observation, tuple):\n",
        "            observation = observation[0]  # Handle new env reset API\n",
        "\n",
        "        # Store original parameters if not stored already\n",
        "        if self.original_stiffness is None and hasattr(self.env.unwrapped, 'model'):\n",
        "            if hasattr(self.env.unwrapped.model, 'geom_kp'):\n",
        "                self.original_stiffness = self.env.unwrapped.model.geom_kp.copy()\n",
        "            if hasattr(self.env.unwrapped.model, 'geom_kd'):\n",
        "                self.original_damping = self.env.unwrapped.model.geom_kd.copy()\n",
        "\n",
        "        # Apply random contact parameters\n",
        "        if hasattr(self.env.unwrapped, 'model'):\n",
        "            # Adjust contact stiffness\n",
        "            if hasattr(self.env.unwrapped.model, 'geom_kp') and self.original_stiffness is not None:\n",
        "                stiffness_factor = np.random.uniform(*self.stiffness_range)\n",
        "                # Apply to ground contact geoms (usually the first few indices)\n",
        "                ground_indices = [0]  # Typically the first geom is the ground\n",
        "                for idx in ground_indices:\n",
        "                    if idx < len(self.original_stiffness):\n",
        "                        self.env.unwrapped.model.geom_kp[idx] = self.original_stiffness[idx] * stiffness_factor\n",
        "\n",
        "            # Adjust contact damping\n",
        "            if hasattr(self.env.unwrapped.model, 'geom_kd') and self.original_damping is not None:\n",
        "                damping_factor = np.random.uniform(*self.damping_range)\n",
        "                for idx in ground_indices:\n",
        "                    if idx < len(self.original_damping):\n",
        "                        self.env.unwrapped.model.geom_kd[idx] = self.original_damping[idx] * damping_factor\n",
        "\n",
        "        return observation\n",
        "\n",
        "# 3.7 Unified Aleatoric Disruption\n",
        "class DisruptionWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, lambda_rate=0.1, intensity_scale=0.05):\n",
        "        super(DisruptionWrapper, self).__init__(env)\n",
        "        self.lambda_rate = lambda_rate\n",
        "        # Calculate intensity vector proportional to observation space shape\n",
        "        state_dim = env.observation_space.shape[0]\n",
        "        self.intensity_vector = np.ones(state_dim) * intensity_scale\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        observation = self.env.reset(**kwargs)\n",
        "        if isinstance(observation, tuple):\n",
        "            observation = observation[0]  # Handle new env reset API\n",
        "        return observation\n",
        "\n",
        "    def step(self, action):\n",
        "        result = self.env.step(action)\n",
        "        if len(result) == 5:\n",
        "            obs, reward, done, truncated, info = result\n",
        "            done = done or truncated\n",
        "        else:\n",
        "            obs, reward, done, info = result\n",
        "\n",
        "        # Apply disruption with probability λ\n",
        "        if np.random.rand() < self.lambda_rate:\n",
        "            noise = np.random.uniform(-self.intensity_vector, self.intensity_vector)\n",
        "            obs = obs + noise\n",
        "\n",
        "        if len(result) == 5:\n",
        "            return obs, reward, done, truncated, info\n",
        "        else:\n",
        "            return obs, reward, done, info\n",
        "\n",
        "# -----------------------------------------\n",
        "# 4. Training function for the PPO agent with GPU support\n",
        "# -----------------------------------------\n",
        "\n",
        "def train_ppo(env_fn, episodes=200, steps_per_update=2048, epochs=10, batch_size=64, gamma=0.99,\n",
        "              lr=3e-4, eps_clip=0.2, value_coef=0.5, entropy_coef=0.01):\n",
        "    env = env_fn()\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "\n",
        "    # Initialize actor-critic network and move to GPU\n",
        "    model = ActorCritic(state_dim, action_dim, max_action).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Initialize buffer\n",
        "    buffer = PPOBuffer(steps_per_update, state_dim, action_dim)\n",
        "\n",
        "    rewards_history = []\n",
        "    running_reward = 0\n",
        "\n",
        "    # Start training\n",
        "    total_steps = 0\n",
        "    episodes_completed = 0\n",
        "\n",
        "    while episodes_completed < episodes:\n",
        "        state = env.reset()\n",
        "        if isinstance(state, tuple):\n",
        "            state = state[0]  # Handle new env reset API\n",
        "\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "\n",
        "        # Episode loop\n",
        "        while not done:\n",
        "            # Convert state to tensor\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "\n",
        "            # Select action\n",
        "            with torch.no_grad():\n",
        "                action, log_prob = model.get_action(state_tensor)\n",
        "                value = model.critic(state_tensor)\n",
        "\n",
        "            # Convert to numpy and take step\n",
        "            action_np = action.cpu().numpy().flatten()\n",
        "\n",
        "            # Take action in environment\n",
        "            step_result = env.step(action_np)\n",
        "            if len(step_result) == 5:\n",
        "                next_state, reward, done, truncated, _ = step_result\n",
        "                done = done or truncated\n",
        "            else:\n",
        "                next_state, reward, done, _ = step_result\n",
        "\n",
        "            # Store transition in buffer\n",
        "            buffer.store(\n",
        "                state,\n",
        "                action_np,\n",
        "                reward,\n",
        "                next_state,\n",
        "                float(done),\n",
        "                log_prob.cpu().numpy()[0],\n",
        "                value.cpu().numpy()[0]\n",
        "            )\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            total_steps += 1\n",
        "\n",
        "            # Update if buffer is full\n",
        "            if buffer.size == steps_per_update:\n",
        "                # Get all data from buffer\n",
        "                states, actions, rewards, next_states, dones, old_log_probs, old_values = buffer.get_all_data()\n",
        "\n",
        "                # Convert to tensors and move to device\n",
        "                states = torch.FloatTensor(states).to(device)\n",
        "                actions = torch.FloatTensor(actions).to(device)\n",
        "                old_log_probs = torch.FloatTensor(old_log_probs).to(device)\n",
        "                old_values = torch.FloatTensor(old_values).to(device)\n",
        "\n",
        "                # Compute returns and advantages\n",
        "                returns = compute_gae(\n",
        "                    rewards,\n",
        "                    dones,\n",
        "                    old_values,\n",
        "                    next_states,\n",
        "                    model,\n",
        "                    gamma=gamma,\n",
        "                    lambda_gae=0.95\n",
        "                )\n",
        "                returns = torch.FloatTensor(returns).to(device)\n",
        "\n",
        "                # Normalize returns (optional but helps with training)\n",
        "                advantages = returns - old_values\n",
        "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "                # Update policy for multiple epochs\n",
        "                for _ in range(epochs):\n",
        "                    # Generate random indices\n",
        "                    indices = torch.randperm(steps_per_update)\n",
        "\n",
        "                    # Create mini-batches\n",
        "                    for start_idx in range(0, steps_per_update, batch_size):\n",
        "                        # Get mini-batch indices\n",
        "                        idx = indices[start_idx:min(start_idx + batch_size, steps_per_update)]\n",
        "\n",
        "                        # Get mini-batch data\n",
        "                        mb_states = states[idx]\n",
        "                        mb_actions = actions[idx]\n",
        "                        mb_old_log_probs = old_log_probs[idx]\n",
        "                        mb_returns = returns[idx]\n",
        "                        mb_advantages = advantages[idx]\n",
        "\n",
        "                        # Get current log probs and values\n",
        "                        new_log_probs, entropy, values = model.evaluate_actions(mb_states, mb_actions)\n",
        "\n",
        "                        # Calculate ratio (π_θ / π_θ_old)\n",
        "                        ratio = torch.exp(new_log_probs - mb_old_log_probs)\n",
        "\n",
        "                        # PPO update\n",
        "                        surr1 = ratio * mb_advantages\n",
        "                        surr2 = torch.clamp(ratio, 1.0 - eps_clip, 1.0 + eps_clip) * mb_advantages\n",
        "\n",
        "                        # Calculate actor loss\n",
        "                        actor_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                        # Calculate critic loss\n",
        "                        critic_loss = nn.MSELoss()(values, mb_returns)\n",
        "\n",
        "                        # Calculate total loss\n",
        "                        loss = actor_loss + value_coef * critic_loss - entropy_coef * entropy\n",
        "\n",
        "                        # Update network\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        # Optional: clip gradients for stability\n",
        "                        nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Clear buffer after update\n",
        "                buffer.clear()\n",
        "\n",
        "        # End of episode processing\n",
        "        episodes_completed += 1\n",
        "        rewards_history.append(episode_reward)\n",
        "\n",
        "        # Exponential moving average for smoother reporting\n",
        "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward if episodes_completed > 1 else episode_reward\n",
        "\n",
        "        if episodes_completed % 10 == 0:\n",
        "            print(f\"Episode {episodes_completed}, Reward: {episode_reward:.2f}, Avg Reward: {running_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    return rewards_history\n",
        "\n",
        "# Helper function for PPO's Generalized Advantage Estimation\n",
        "def compute_gae(rewards, dones, values, next_states, model, gamma=0.99, lambda_gae=0.95):\n",
        "    returns = []\n",
        "    gae = 0\n",
        "\n",
        "    # Move the model to evaluation mode for inference\n",
        "    with torch.no_grad():\n",
        "        # Get next state values for the last state\n",
        "        next_state = torch.FloatTensor(next_states[-1]).unsqueeze(0).to(device)\n",
        "        next_value = model.critic(next_state).cpu().numpy()[0, 0]\n",
        "\n",
        "    # Append next value to values for easier computation\n",
        "    values = np.append(values, next_value)\n",
        "\n",
        "    # Compute returns with GAE\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        if step == len(rewards) - 1:\n",
        "            next_non_terminal = 1.0 - dones[step]\n",
        "            next_return = next_value\n",
        "        else:\n",
        "            next_non_terminal = 1.0 - dones[step]\n",
        "            next_return = returns[0]\n",
        "\n",
        "        # Calculate delta and GAE\n",
        "        delta = rewards[step] + gamma * values[step + 1] * next_non_terminal - values[step]\n",
        "        gae = delta + gamma * lambda_gae * next_non_terminal * gae\n",
        "\n",
        "        # Insert at the beginning (as we're going backwards)\n",
        "        returns.insert(0, gae + values[step])\n",
        "\n",
        "    return np.array(returns)\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# 5. Create environment functions for each uncertainty (unchanged)\n",
        "# ------------------------------------------------------\n",
        "\n",
        "def make_base_env():\n",
        "    # Base environment (no uncertainty)\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    return env\n",
        "\n",
        "def make_sensor_noise_env():\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    env = SensorNoiseWrapper(env, noise_std=0.01)\n",
        "    return env\n",
        "\n",
        "def make_motor_noise_env():\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    env = MotorNoiseWrapper(env, noise_std=0.05)\n",
        "    return env\n",
        "\n",
        "def make_mass_variability_env():\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    env = MassVariabilityWrapper(env, mass_variation_range=(0.8, 1.2))\n",
        "    return env\n",
        "\n",
        "def make_terrain_resistance_env():\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    env = TerrainResistanceWrapper(env, drag_range=(0.0, 0.3))\n",
        "    return env\n",
        "\n",
        "def make_external_force_env():\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    env = ExternalForceWrapper(env, force_magnitude_range=(-100.0, 100.0), pulse_probability=0.05)\n",
        "    return env\n",
        "\n",
        "def make_variable_contact_env():\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    env = VariableContactWrapper(env, stiffness_range=(1.0, 10.0), damping_range=(0.1, 1.0))\n",
        "    return env\n",
        "\n",
        "def make_disruption_env():\n",
        "    env = gym.make('HalfCheetah-v4')\n",
        "    env = DisruptionWrapper(env, lambda_rate=0.1, intensity_scale=0.05)\n",
        "    return env\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 6. Run experiments and collect performance metrics\n",
        "# -------------------------------------------------\n",
        "\n",
        "# Define the uncertainty experiments\n",
        "experiments = {\n",
        "    \"Base\": make_base_env,\n",
        "    \"Sensor Noise\": make_sensor_noise_env,\n",
        "    \"Motor Noise\": make_motor_noise_env,\n",
        "    \"Mass Variability\": make_mass_variability_env,\n",
        "    \"Terrain Resistance\": make_terrain_resistance_env,\n",
        "    \"External Force\": make_external_force_env,\n",
        "    \"Variable Contact\": make_variable_contact_env,\n",
        "    \"Disruption Model\": make_disruption_env\n",
        "}\n",
        "\n",
        "results = {}\n",
        "episodes = 300  # Adjust as needed for HalfCheetah\n",
        "\n",
        "for key, env_fn in experiments.items():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training with {key} uncertainty:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    rewards = train_ppo(env_fn, episodes=episodes)\n",
        "    results[key] = rewards\n",
        "\n",
        "    # Save intermediate results after each experiment (in case of crash)\n",
        "    np.save(f\"rewards_ppo_{key.replace(' ', '_').lower()}.npy\", np.array(rewards))\n",
        "\n",
        "# --------------------------------------\n",
        "# 7. Plot the training performance curves with improved visualization\n",
        "# --------------------------------------\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Apply smoothing for clearer visualization\n",
        "window_size = 20\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(experiments)))\n",
        "\n",
        "for i, (key, rewards) in enumerate(results.items()):\n",
        "    # Compute rolling average for smoother curves\n",
        "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "    # Plot both raw (light) and smoothed (dark) curves\n",
        "    plt.plot(rewards, alpha=0.3, color=colors[i])\n",
        "    plt.plot(range(window_size-1, len(rewards)), smoothed_rewards,\n",
        "             label=key, linewidth=2, color=colors[i])\n",
        "\n",
        "plt.xlabel(\"Episode\", fontsize=14)\n",
        "plt.ylabel(\"Reward\", fontsize=14)\n",
        "plt.title(\"PPO Performance with Different Aleatoric Uncertainties in HalfCheetah\", fontsize=16)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the final plot\n",
        "plt.savefig(\"halfcheetah_ppo_uncertainty_performance.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Also save all results together\n",
        "np.save(\"ppo_all_results.npy\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4hIJ00PkY8fk",
        "outputId": "b0215913-19a0-4dfc-b714-a813ab933804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "\n",
            "==================================================\n",
            "Training with Base uncertainty:\n",
            "==================================================\n",
            "Episode 10, Reward: -289.45, Avg Reward: -215.18\n",
            "Episode 20, Reward: -244.74, Avg Reward: -213.28\n",
            "Episode 30, Reward: -45.57, Avg Reward: -160.05\n",
            "Episode 40, Reward: -15.76, Avg Reward: -126.73\n",
            "Episode 50, Reward: 124.03, Avg Reward: -79.37\n",
            "Episode 60, Reward: 155.23, Avg Reward: -1.30\n",
            "Episode 70, Reward: 349.17, Avg Reward: 107.56\n",
            "Episode 80, Reward: 287.27, Avg Reward: 143.82\n",
            "Episode 90, Reward: 395.63, Avg Reward: 219.51\n",
            "Episode 100, Reward: 433.63, Avg Reward: 316.29\n",
            "Episode 110, Reward: 616.57, Avg Reward: 422.32\n",
            "Episode 120, Reward: 804.56, Avg Reward: 549.84\n",
            "Episode 130, Reward: 353.90, Avg Reward: 597.83\n",
            "Episode 140, Reward: -6.49, Avg Reward: 284.86\n",
            "Episode 150, Reward: 317.17, Avg Reward: 267.44\n",
            "Episode 160, Reward: -365.38, Avg Reward: 173.02\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected parameter loc (Tensor of shape (64, 6)) of distribution Normal(loc: torch.Size([64, 6]), scale: torch.Size([64, 6])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan]], grad_fn=<AddmmBackward0>)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2a9bd74611e9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training with {key} uncertainty:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{'='*50}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2a9bd74611e9>\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m(env_fn, episodes, steps_per_update, epochs, batch_size, gamma, lr, eps_clip, value_coef, entropy_coef)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                         \u001b[0;31m# Get current log probs and values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                         \u001b[0mnew_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                         \u001b[0;31m# Calculate ratio (π_θ / π_θ_old)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2a9bd74611e9>\u001b[0m in \u001b[0;36mevaluate_actions\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Get log probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m     72\u001b[0m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                         \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (64, 6)) of distribution Normal(loc: torch.Size([64, 6]), scale: torch.Size([64, 6])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan]], grad_fn=<AddmmBackward0>)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Dictionary of uncertainty types with (episodes, avg_rewards)\n",
        "data = {\n",
        "    \"Base\": [(0, -403.41), (10, -392.77), (20, -287.14), (30, -246.83), (40, 36.51), (50, 206.67), (60, 732.69),\n",
        "             (70, 928.09), (80, 1370.82), (90, 1738.97), (100, 1918.93), (110, 2141.82), (120, 2413.21),\n",
        "             (130, 2506.96), (140, 2684.61), (150, 2932.41), (160, 3123.88), (170, 3182.31), (180, 3297.13),\n",
        "             (190, 3417.39), (200, 3392.55), (210, 3566.15), (220, 3738.76), (230, 3803.97), (240, 3883.50),\n",
        "             (250, 3958.00), (260, 4075.48), (270, 4133.07), (280, 4104.00), (290, 3814.03)],\n",
        "    \"Sensor Noise\": [(0, -380.85), (10, -349.44), (20, -276.75), (30, -127.02), (40, 468.70), (50, 985.00),\n",
        "                     (60, 1289.26), (70, 1465.01), (80, 1607.54), (90, 1722.27), (100, 1795.15), (110, 1860.37),\n",
        "                     (120, 1809.63), (130, 1923.07), (140, 2033.93), (150, 2114.41), (160, 2196.60), (170, 2286.50),\n",
        "                     (180, 2289.15), (190, 2395.71), (200, 2412.61), (210, 2408.48), (220, 2368.16), (230, 2339.87),\n",
        "                     (240, 2288.26), (250, 2480.20), (260, 2603.88), (270, 2642.46), (280, 2723.79), (290, 2618.82)],\n",
        "    \"Motor Noise\": [(0, -321.69), (10, -244.99), (20, 5.34), (30, 207.40), (40, 314.76), (50, 458.00), (60, 579.96),\n",
        "                    (70, 652.30), (80, 698.90), (90, 730.16), (100, 746.66), (110, 776.35), (120, 794.49),\n",
        "                    (130, 802.37), (140, 807.90), (150, 815.09), (160, 813.62), (170, 809.75), (180, 811.71),\n",
        "                    (190, 813.07), (200, 818.22), (210, 833.04), (220, 830.11), (230, 818.08), (240, 818.03),\n",
        "                    (250, 825.12), (260, 835.86), (270, 839.46), (280, 850.18), (290, 857.21)],\n",
        "    \"Mass Variability\": [(0, -361.93), (10, -352.14), (20, -256.71), (30, -16.93), (40, 689.54), (50, 1245.79),\n",
        "                         (60, 1570.04), (70, 1789.35), (80, 1943.40), (90, 2016.20), (100, 1867.70), (110, 1860.02),\n",
        "                         (120, 1860.76), (130, 1998.81), (140, 2081.14), (150, 2127.60), (160, 2174.44), (170, 2206.16),\n",
        "                         (180, 2259.16), (190, 2290.86), (200, 2319.21), (210, 2359.40), (220, 2415.85), (230, 2522.84),\n",
        "                         (240, 2602.94), (250, 2661.24), (260, 2655.47), (270, 2772.77), (280, 2841.44), (290, 2893.09)],\n",
        "    \"Terrain Resistance\": [(0, -443.98), (10, -356.10), (20, -218.21), (30, 109.95), (40, 462.56), (50, 746.17),\n",
        "                           (60, 929.23), (70, 1041.92), (80, 1123.98), (90, 1149.48), (100, 1164.29), (110, 1058.88),\n",
        "                           (120, 1039.60), (130, 1090.45), (140, 1115.04), (150, 1149.91), (160, 1152.57), (170, 1156.56),\n",
        "                           (180, 1146.68), (190, 1170.77), (200, 1180.02), (210, 1183.58), (220, 1192.49),\n",
        "                           (230, 1196.09), (240, 1188.89), (250, 1191.24), (260, 1195.59), (270, 1192.84),\n",
        "                           (280, 1191.36), (290, 1186.93)],\n",
        "    \"External Force\": [(0, -406.79), (10, -371.34), (20, -250.22), (30, -98.19), (40, 4.60), (50, 97.17), (60, 134.67),\n",
        "                       (70, 198.55), (80, 223.08), (90, 219.07), (100, 221.90), (110, 221.34), (120, 223.98),\n",
        "                       (130, 248.91), (140, 265.64), (150, 289.92), (160, 320.47), (170, 380.70), (180, 428.47),\n",
        "                       (190, 505.26), (200, 556.69), (210, 624.11), (220, 690.75), (230, 812.98), (240, 885.20),\n",
        "                       (250, 953.04), (260, 1018.39), (270, 1084.23), (280, 1126.02), (290, 1099.47)],\n",
        "    \"Variable Contact\": [(0, -339.55), (10, -197.77), (20, 169.70), (30, 562.16), (40, 906.32), (50, 1178.07),\n",
        "                         (60, 1415.95), (70, 1591.06), (80, 1689.16), (90, 1739.29), (100, 1777.64), (110, 1803.08),\n",
        "                         (120, 1831.42), (130, 1826.62), (140, 1848.29), (150, 1848.35), (160, 1820.74), (170, 1789.79),\n",
        "                         (180, 1768.45), (190, 1765.02), (200, 1801.99), (210, 1841.32), (220, 1909.43), (230, 1893.20),\n",
        "                         (240, 1954.00), (250, 1982.59), (260, 2025.75), (270, 2068.74), (280, 2129.68), (290, 2185.52)],\n",
        "    \"Disruption Model\": [(0, -383.76), (10, -335.92), (20, -116.51), (30, 314.94), (40, 923.13), (50, 1341.56),\n",
        "                         (60, 1584.48), (70, 1798.72), (80, 1941.30), (90, 2070.86), (100, 2181.62), (110, 2297.78),\n",
        "                         (120, 2444.02), (130, 2595.23), (140, 2692.45), (150, 2833.48), (160, 2977.72), (170, 3016.02)]\n",
        "}\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure()\n",
        "\n",
        "for label, values in data.items():\n",
        "    episodes, rewards = zip(*values)\n",
        "    fig.add_trace(go.Scatter(x=episodes, y=rewards, mode='lines', name=label))\n",
        "\n",
        "# Layout customization\n",
        "fig.update_layout(\n",
        "    title=\"Training Performance under Different Uncertainties\",\n",
        "    xaxis_title=\"Episodes\",\n",
        "    yaxis_title=\"Average Reward\",\n",
        "    plot_bgcolor=\"white\",\n",
        "    legend=dict(font=dict(size=10)),\n",
        "    margin=dict(l=40, r=40, t=60, b=40),\n",
        ")\n",
        "\n",
        "# Add grid lines\n",
        "fig.update_xaxes(showgrid=True, gridcolor='lightgray')\n",
        "fig.update_yaxes(showgrid=True, gridcolor='lightgray')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Oid8d0rlno7E",
        "outputId": "18a8edb3-9cc7-43d5-8f5a-11dc0d7089e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"d36dd388-67cc-4394-8766-561b86e34b1f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d36dd388-67cc-4394-8766-561b86e34b1f\")) {                    Plotly.newPlot(                        \"d36dd388-67cc-4394-8766-561b86e34b1f\",                        [{\"mode\":\"lines\",\"name\":\"Base\",\"x\":[0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290],\"y\":[-403.41,-392.77,-287.14,-246.83,36.51,206.67,732.69,928.09,1370.82,1738.97,1918.93,2141.82,2413.21,2506.96,2684.61,2932.41,3123.88,3182.31,3297.13,3417.39,3392.55,3566.15,3738.76,3803.97,3883.5,3958.0,4075.48,4133.07,4104.0,3814.03],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Sensor Noise\",\"x\":[0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290],\"y\":[-380.85,-349.44,-276.75,-127.02,468.7,985.0,1289.26,1465.01,1607.54,1722.27,1795.15,1860.37,1809.63,1923.07,2033.93,2114.41,2196.6,2286.5,2289.15,2395.71,2412.61,2408.48,2368.16,2339.87,2288.26,2480.2,2603.88,2642.46,2723.79,2618.82],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Motor Noise\",\"x\":[0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290],\"y\":[-321.69,-244.99,5.34,207.4,314.76,458.0,579.96,652.3,698.9,730.16,746.66,776.35,794.49,802.37,807.9,815.09,813.62,809.75,811.71,813.07,818.22,833.04,830.11,818.08,818.03,825.12,835.86,839.46,850.18,857.21],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Mass Variability\",\"x\":[0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290],\"y\":[-361.93,-352.14,-256.71,-16.93,689.54,1245.79,1570.04,1789.35,1943.4,2016.2,1867.7,1860.02,1860.76,1998.81,2081.14,2127.6,2174.44,2206.16,2259.16,2290.86,2319.21,2359.4,2415.85,2522.84,2602.94,2661.24,2655.47,2772.77,2841.44,2893.09],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Terrain Resistance\",\"x\":[0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290],\"y\":[-443.98,-356.1,-218.21,109.95,462.56,746.17,929.23,1041.92,1123.98,1149.48,1164.29,1058.88,1039.6,1090.45,1115.04,1149.91,1152.57,1156.56,1146.68,1170.77,1180.02,1183.58,1192.49,1196.09,1188.89,1191.24,1195.59,1192.84,1191.36,1186.93],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"External Force\",\"x\":[0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290],\"y\":[-406.79,-371.34,-250.22,-98.19,4.6,97.17,134.67,198.55,223.08,219.07,221.9,221.34,223.98,248.91,265.64,289.92,320.47,380.7,428.47,505.26,556.69,624.11,690.75,812.98,885.2,953.04,1018.39,1084.23,1126.02,1099.47],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Variable Contact\",\"x\":[0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290],\"y\":[-339.55,-197.77,169.7,562.16,906.32,1178.07,1415.95,1591.06,1689.16,1739.29,1777.64,1803.08,1831.42,1826.62,1848.29,1848.35,1820.74,1789.79,1768.45,1765.02,1801.99,1841.32,1909.43,1893.2,1954.0,1982.59,2025.75,2068.74,2129.68,2185.52],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Disruption Model\",\"x\":[0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170],\"y\":[-383.76,-335.92,-116.51,314.94,923.13,1341.56,1584.48,1798.72,1941.3,2070.86,2181.62,2297.78,2444.02,2595.23,2692.45,2833.48,2977.72,3016.02],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"legend\":{\"font\":{\"size\":10}},\"margin\":{\"l\":40,\"r\":40,\"t\":60,\"b\":40},\"title\":{\"text\":\"Training Performance under Different Uncertainties\"},\"xaxis\":{\"title\":{\"text\":\"Episodes\"},\"showgrid\":true,\"gridcolor\":\"lightgray\"},\"yaxis\":{\"title\":{\"text\":\"Average Reward\"},\"showgrid\":true,\"gridcolor\":\"lightgray\"},\"plot_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d36dd388-67cc-4394-8766-561b86e34b1f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "icDl4p3A-RvE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}