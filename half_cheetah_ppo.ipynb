{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d80bb60-d795-4f46-9ebe-3d363333df8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: glfw in ./.local/lib/python3.11/site-packages (2.8.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mujoco in ./.local/lib/python3.11/site-packages (3.3.0)\n",
      "Requirement already satisfied: absl-py in /shared/courseSharedFolders/142601outer/142601/cs109b/lib/python3.11/site-packages (from mujoco) (2.1.0)\n",
      "Requirement already satisfied: etils[epath] in /shared/courseSharedFolders/142601outer/142601/cs109b/lib/python3.11/site-packages (from mujoco) (1.12.1)\n",
      "Requirement already satisfied: glfw in ./.local/lib/python3.11/site-packages (from mujoco) (2.8.0)\n",
      "Requirement already satisfied: numpy in /shared/courseSharedFolders/142601outer/142601/cs109b/lib/python3.11/site-packages (from mujoco) (1.26.3)\n",
      "Requirement already satisfied: pyopengl in ./.local/lib/python3.11/site-packages (from mujoco) (3.1.9)\n",
      "Requirement already satisfied: fsspec in /shared/courseSharedFolders/142601outer/142601/cs109b/lib/python3.11/site-packages (from etils[epath]->mujoco) (2024.3.1)\n",
      "Requirement already satisfied: importlib_resources in /shared/courseSharedFolders/142601outer/142601/cs109b/lib/python3.11/site-packages (from etils[epath]->mujoco) (6.5.2)\n",
      "Requirement already satisfied: typing_extensions in /shared/courseSharedFolders/142601outer/142601/cs109b/lib/python3.11/site-packages (from etils[epath]->mujoco) (4.12.2)\n",
      "Requirement already satisfied: zipp in /shared/courseSharedFolders/142601outer/142601/cs109b/lib/python3.11/site-packages (from etils[epath]->mujoco) (3.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install glfw\n",
    "!pip install mujoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3480f602-e997-4dc3-b7b8-13c37741787c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4514/425737094.py:2: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not hasattr(np, 'bool8'):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "if not hasattr(np, 'bool8'):\n",
    "    np.bool8 = np.bool_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa847b2-5dcf-4d72-86b7-e58c5fab068e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==================================================\n",
      "Training with Base uncertainty:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/home/kut385/.local/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 602\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m uncertainty:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 602\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m results[key] \u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# Save intermediate results after each experiment (in case of crash)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 429\u001b[0m, in \u001b[0;36mtrain_ppo\u001b[0;34m(env_fn, episodes, steps_per_update, epochs, batch_size, gamma, lr, eps_clip, value_coef, entropy_coef)\u001b[0m\n\u001b[1;32m    426\u001b[0m old_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(old_values)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# Compute returns and advantages\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m returns \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mold_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_gae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\n\u001b[1;32m    437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m returns \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(returns)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Normalize returns (optional but helps with training)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 515\u001b[0m, in \u001b[0;36mcompute_gae\u001b[0;34m(rewards, dones, values, next_states, model, gamma, lambda_gae)\u001b[0m\n\u001b[1;32m    512\u001b[0m     next_value \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcritic(next_state)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# Append next value to values for easier computation\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# Compute returns with GAE\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rewards))):\n",
      "File \u001b[0;32m/shared/courseSharedFolders/142601outer/142601/cs109b/lib/python3.11/site-packages/numpy/lib/function_base.py:5611\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5562\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_append_dispatcher)\n\u001b[1;32m   5563\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mappend\u001b[39m(arr, values, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   5564\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5565\u001b[0m \u001b[38;5;124;03m    Append values to the end of an array.\u001b[39;00m\n\u001b[1;32m   5566\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5609\u001b[0m \n\u001b[1;32m   5610\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5611\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5613\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/shared/courseSharedFolders/142601outer/142601/cs109b/lib/python3.11/site-packages/torch/_tensor.py:1030\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------\n",
    "# 1. GPU Configuration\n",
    "# ------------------------------\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Define the PPO components for continuous action spaces\n",
    "# ------------------------------\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # Shared network layers\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor (policy) network\n",
    "        self.mean = nn.Linear(256, action_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "        \n",
    "        # # Critic (value) network\n",
    "        # self.critic = nn.Linear(256, 1)\n",
    "\n",
    "        # Critic (value) network - rename to value_layer\n",
    "        self.value_layer = nn.Linear(256, 1)  # Renamed from \"critic\"\n",
    "        \n",
    "        # Action scale\n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def forward(self):\n",
    "        # Not used directly\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def actor(self, state):\n",
    "        x = self.shared(state)\n",
    "        mean = self.mean(x)\n",
    "        log_std = self.log_std.expand_as(mean)\n",
    "        # Clamp log_std to prevent very small or large values\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        return mean, std\n",
    "        \n",
    "    def critic(self, state):\n",
    "        x = self.shared(state)\n",
    "        value = self.value_layer(x)  # Use the renamed layer\n",
    "        #value = self.critic(x)\n",
    "        return value\n",
    "    \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        mean, std = self.actor(state)\n",
    "        \n",
    "        if deterministic:\n",
    "            return self.max_action * torch.tanh(mean)\n",
    "        \n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        # Tanh squashing for bounded actions\n",
    "        action = self.max_action * torch.tanh(action)\n",
    "        \n",
    "        # Calculate log probability for the sampled action\n",
    "        # We need to account for the tanh transformation when computing log probs\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Apply tanh to keep actions within bounds\n",
    "        return action, log_prob\n",
    "    \n",
    "    def evaluate_actions(self, state, action):\n",
    "        mean, std = self.actor(state)\n",
    "        dist = Normal(mean, std)\n",
    "        \n",
    "        # Get log probability\n",
    "        log_probs = dist.log_prob(action).sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Get entropy for exploration\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        # Get state value\n",
    "        value = self.critic(state)\n",
    "        \n",
    "        return log_probs, entropy, value\n",
    "\n",
    "# Simple buffer for PPO\n",
    "class PPOBuffer:\n",
    "    def __init__(self, capacity, state_dim, action_dim):\n",
    "        self.states = np.zeros((capacity, state_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n",
    "        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)\n",
    "        self.dones = np.zeros((capacity, 1), dtype=np.float32)\n",
    "        self.log_probs = np.zeros((capacity, 1), dtype=np.float32)\n",
    "        self.values = np.zeros((capacity, 1), dtype=np.float32)\n",
    "        \n",
    "        self.idx = 0\n",
    "        self.size = 0\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def store(self, state, action, reward, next_state, done, log_prob, value):\n",
    "        self.states[self.idx] = state\n",
    "        self.actions[self.idx] = action\n",
    "        self.rewards[self.idx] = reward\n",
    "        self.next_states[self.idx] = next_state\n",
    "        self.dones[self.idx] = done\n",
    "        self.log_probs[self.idx] = log_prob\n",
    "        self.values[self.idx] = value\n",
    "        \n",
    "        self.idx = (self.idx + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "    \n",
    "    def get_all_data(self):\n",
    "        return (\n",
    "            self.states[:self.size],\n",
    "            self.actions[:self.size],\n",
    "            self.rewards[:self.size],\n",
    "            self.next_states[:self.size],\n",
    "            self.dones[:self.size],\n",
    "            self.log_probs[:self.size],\n",
    "            self.values[:self.size]\n",
    "        )\n",
    "    \n",
    "    def clear(self):\n",
    "        self.idx = 0\n",
    "        self.size = 0\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. Define wrappers for uncertainties (unchanged)\n",
    "# -----------------------------------\n",
    "\n",
    "# 3.1 Sensor/Observation Noise: add Gaussian noise to observations\n",
    "class SensorNoiseWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, noise_std=0.01):\n",
    "        super(SensorNoiseWrapper, self).__init__(env)\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def observation(self, observation):\n",
    "        noise = np.random.normal(0, self.noise_std, size=observation.shape)\n",
    "        return observation + noise\n",
    "\n",
    "# 3.2 Motor Noise: add random noise to actions (torque outputs)\n",
    "class MotorNoiseWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, noise_std=0.05):\n",
    "        super(MotorNoiseWrapper, self).__init__(env)\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def action(self, action):\n",
    "        noise = np.random.normal(0, self.noise_std, size=action.shape)\n",
    "        return np.clip(action + noise, self.action_space.low, self.action_space.high)\n",
    "\n",
    "# 3.3 Leg Mass or Joint Stiffness Variation\n",
    "class MassVariabilityWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, mass_variation_range=(0.8, 1.2)):\n",
    "        super(MassVariabilityWrapper, self).__init__(env)\n",
    "        self.mass_variation_range = mass_variation_range\n",
    "        self.original_body_mass = None\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        if isinstance(observation, tuple):\n",
    "            observation = observation[0]  # Handle new env reset API\n",
    "        \n",
    "        # Store original masses if not stored already\n",
    "        if self.original_body_mass is None and hasattr(self.env.unwrapped, 'model'):\n",
    "            self.original_body_mass = self.env.unwrapped.model.body_mass.copy()\n",
    "        \n",
    "        # Apply random mass variations if model exists\n",
    "        if hasattr(self.env.unwrapped, 'model') and self.original_body_mass is not None:\n",
    "            # Focus on leg masses (depending on the specific model structure)\n",
    "            # Indices 4-8 typically correspond to the leg parts in HalfCheetah\n",
    "            leg_indices = range(4, 9)  # Adjust based on actual model\n",
    "            \n",
    "            for idx in leg_indices:\n",
    "                if idx < len(self.original_body_mass):\n",
    "                    variation = np.random.uniform(*self.mass_variation_range)\n",
    "                    self.env.unwrapped.model.body_mass[idx] = self.original_body_mass[idx] * variation\n",
    "        \n",
    "        return observation\n",
    "\n",
    "# 3.4 Random Drag or Terrain Resistance\n",
    "class TerrainResistanceWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, drag_range=(0.0, 0.3)):\n",
    "        super(TerrainResistanceWrapper, self).__init__(env)\n",
    "        self.drag_range = drag_range\n",
    "        self.current_drag = 0.0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        if isinstance(observation, tuple):\n",
    "            observation = observation[0]  # Handle new env reset API\n",
    "        \n",
    "        # Set a new random drag coefficient for this episode\n",
    "        self.current_drag = np.random.uniform(*self.drag_range)\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        result = self.env.step(action)\n",
    "        \n",
    "        # Handle both the new_step_api (5 values) and old (4 values) cases\n",
    "        if len(result) == 5:\n",
    "            obs, reward, done, truncated, info = result\n",
    "            done = done or truncated\n",
    "        else:\n",
    "            obs, reward, done, info = result\n",
    "        \n",
    "        # Apply drag by modifying velocity components if we can access them\n",
    "        if hasattr(self.env.unwrapped, 'sim'):\n",
    "            # Get current velocities\n",
    "            qvel = self.env.unwrapped.sim.data.qvel.copy()\n",
    "            \n",
    "            # Apply drag to horizontal velocity (typically the first velocity component)\n",
    "            if len(qvel) > 0:\n",
    "                qvel[0] *= (1.0 - self.current_drag)\n",
    "                \n",
    "                # Update velocities in the simulation\n",
    "                self.env.unwrapped.sim.data.qvel[:] = qvel\n",
    "        \n",
    "        if len(result) == 5:\n",
    "            return obs, reward, done, truncated, info\n",
    "        else:\n",
    "            return obs, reward, done, info\n",
    "\n",
    "# 3.5 External Force Pulses\n",
    "class ExternalForceWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, force_magnitude_range=(-100.0, 100.0), pulse_probability=0.05):\n",
    "        super(ExternalForceWrapper, self).__init__(env)\n",
    "        self.force_magnitude_range = force_magnitude_range\n",
    "        self.pulse_probability = pulse_probability\n",
    "\n",
    "    def step(self, action):\n",
    "        result = self.env.step(action)\n",
    "        \n",
    "        # Handle both return formats\n",
    "        if len(result) == 5:\n",
    "            obs, reward, done, truncated, info = result\n",
    "            done = done or truncated\n",
    "        else:\n",
    "            obs, reward, done, info = result\n",
    "        \n",
    "        # Randomly apply force pulses\n",
    "        if np.random.rand() < self.pulse_probability and hasattr(self.env.unwrapped, 'sim'):\n",
    "            # Choose a random force magnitude\n",
    "            force_magnitude = np.random.uniform(*self.force_magnitude_range)\n",
    "            \n",
    "            # Apply the force to the torso (typically body index 1)\n",
    "            torso_idx = 1  # Adjust based on actual model\n",
    "            if hasattr(self.env.unwrapped.sim, 'data'):\n",
    "                # Create force vector [x, y, z] - mainly in x direction (forward/backward)\n",
    "                force = np.array([force_magnitude, 0.0, 0.0])\n",
    "                \n",
    "                # Apply the force if the method exists\n",
    "                if hasattr(self.env.unwrapped.sim, 'apply_force'):\n",
    "                    self.env.unwrapped.sim.apply_force(force, torso_idx)\n",
    "        \n",
    "        if len(result) == 5:\n",
    "            return obs, reward, done, truncated, info\n",
    "        else:\n",
    "            return obs, reward, done, info\n",
    "\n",
    "# 3.6 Variable Contact Softness\n",
    "class VariableContactWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, stiffness_range=(1.0, 10.0), damping_range=(0.1, 1.0)):\n",
    "        super(VariableContactWrapper, self).__init__(env)\n",
    "        self.stiffness_range = stiffness_range\n",
    "        self.damping_range = damping_range\n",
    "        self.original_stiffness = None\n",
    "        self.original_damping = None\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        if isinstance(observation, tuple):\n",
    "            observation = observation[0]  # Handle new env reset API\n",
    "        \n",
    "        # Store original parameters if not stored already\n",
    "        if self.original_stiffness is None and hasattr(self.env.unwrapped, 'model'):\n",
    "            if hasattr(self.env.unwrapped.model, 'geom_kp'):\n",
    "                self.original_stiffness = self.env.unwrapped.model.geom_kp.copy()\n",
    "            if hasattr(self.env.unwrapped.model, 'geom_kd'):\n",
    "                self.original_damping = self.env.unwrapped.model.geom_kd.copy()\n",
    "        \n",
    "        # Apply random contact parameters\n",
    "        if hasattr(self.env.unwrapped, 'model'):\n",
    "            # Adjust contact stiffness\n",
    "            if hasattr(self.env.unwrapped.model, 'geom_kp') and self.original_stiffness is not None:\n",
    "                stiffness_factor = np.random.uniform(*self.stiffness_range)\n",
    "                # Apply to ground contact geoms (usually the first few indices)\n",
    "                ground_indices = [0]  # Typically the first geom is the ground\n",
    "                for idx in ground_indices:\n",
    "                    if idx < len(self.original_stiffness):\n",
    "                        self.env.unwrapped.model.geom_kp[idx] = self.original_stiffness[idx] * stiffness_factor\n",
    "            \n",
    "            # Adjust contact damping\n",
    "            if hasattr(self.env.unwrapped.model, 'geom_kd') and self.original_damping is not None:\n",
    "                damping_factor = np.random.uniform(*self.damping_range)\n",
    "                for idx in ground_indices:\n",
    "                    if idx < len(self.original_damping):\n",
    "                        self.env.unwrapped.model.geom_kd[idx] = self.original_damping[idx] * damping_factor\n",
    "        \n",
    "        return observation\n",
    "\n",
    "# 3.7 Unified Aleatoric Disruption\n",
    "class DisruptionWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, lambda_rate=0.1, intensity_scale=0.05):\n",
    "        super(DisruptionWrapper, self).__init__(env)\n",
    "        self.lambda_rate = lambda_rate\n",
    "        # Calculate intensity vector proportional to observation space shape\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        self.intensity_vector = np.ones(state_dim) * intensity_scale\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        if isinstance(observation, tuple):\n",
    "            observation = observation[0]  # Handle new env reset API\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        result = self.env.step(action)\n",
    "        if len(result) == 5:\n",
    "            obs, reward, done, truncated, info = result\n",
    "            done = done or truncated\n",
    "        else:\n",
    "            obs, reward, done, info = result\n",
    "\n",
    "        # Apply disruption with probability λ\n",
    "        if np.random.rand() < self.lambda_rate:\n",
    "            noise = np.random.uniform(-self.intensity_vector, self.intensity_vector)\n",
    "            obs = obs + noise\n",
    "\n",
    "        if len(result) == 5:\n",
    "            return obs, reward, done, truncated, info\n",
    "        else:\n",
    "            return obs, reward, done, info\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4. Training function for the PPO agent with GPU support\n",
    "# -----------------------------------------\n",
    "\n",
    "def train_ppo(env_fn, episodes=200, steps_per_update=2048, epochs=10, batch_size=64, gamma=0.99, \n",
    "              lr=3e-4, eps_clip=0.2, value_coef=0.5, entropy_coef=0.01):\n",
    "    env = env_fn()\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "    \n",
    "    # Initialize actor-critic network and move to GPU\n",
    "    model = ActorCritic(state_dim, action_dim, max_action).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Initialize buffer\n",
    "    buffer = PPOBuffer(steps_per_update, state_dim, action_dim)\n",
    "    \n",
    "    rewards_history = []\n",
    "    running_reward = 0\n",
    "    \n",
    "    # Start training\n",
    "    total_steps = 0\n",
    "    episodes_completed = 0\n",
    "    \n",
    "    while episodes_completed < episodes:\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Handle new env reset API\n",
    "        \n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # Episode loop\n",
    "        while not done:\n",
    "            # Convert state to tensor\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Select action\n",
    "            with torch.no_grad():\n",
    "                action, log_prob = model.get_action(state_tensor)\n",
    "                value = model.critic(state_tensor)\n",
    "            \n",
    "            # Convert to numpy and take step\n",
    "            action_np = action.cpu().numpy().flatten()\n",
    "            \n",
    "            # Take action in environment\n",
    "            step_result = env.step(action_np)\n",
    "            if len(step_result) == 5:\n",
    "                next_state, reward, done, truncated, _ = step_result\n",
    "                done = done or truncated\n",
    "            else:\n",
    "                next_state, reward, done, _ = step_result\n",
    "            \n",
    "            # Store transition in buffer\n",
    "            buffer.store(\n",
    "                state, \n",
    "                action_np, \n",
    "                reward, \n",
    "                next_state, \n",
    "                float(done), \n",
    "                log_prob.cpu().numpy()[0], \n",
    "                value.cpu().numpy()[0]\n",
    "            )\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Update if buffer is full\n",
    "            if buffer.size == steps_per_update:\n",
    "                # Get all data from buffer\n",
    "                states, actions, rewards, next_states, dones, old_log_probs, old_values = buffer.get_all_data()\n",
    "                \n",
    "                # Convert to tensors and move to device\n",
    "                states = torch.FloatTensor(states).to(device)\n",
    "                actions = torch.FloatTensor(actions).to(device)\n",
    "                old_log_probs = torch.FloatTensor(old_log_probs).to(device)\n",
    "                old_values = torch.FloatTensor(old_values).to(device)\n",
    "                \n",
    "                # Compute returns and advantages\n",
    "                returns = compute_gae(\n",
    "                    rewards, \n",
    "                    dones, \n",
    "                    old_values, \n",
    "                    next_states, \n",
    "                    model, \n",
    "                    gamma=gamma, \n",
    "                    lambda_gae=0.95\n",
    "                )\n",
    "                returns = torch.FloatTensor(returns).to(device)\n",
    "                \n",
    "                # Normalize returns (optional but helps with training)\n",
    "                advantages = returns - old_values\n",
    "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "                \n",
    "                # Update policy for multiple epochs\n",
    "                for _ in range(epochs):\n",
    "                    # Generate random indices\n",
    "                    indices = torch.randperm(steps_per_update)\n",
    "                    \n",
    "                    # Create mini-batches\n",
    "                    for start_idx in range(0, steps_per_update, batch_size):\n",
    "                        # Get mini-batch indices\n",
    "                        idx = indices[start_idx:min(start_idx + batch_size, steps_per_update)]\n",
    "                        \n",
    "                        # Get mini-batch data\n",
    "                        mb_states = states[idx]\n",
    "                        mb_actions = actions[idx]\n",
    "                        mb_old_log_probs = old_log_probs[idx]\n",
    "                        mb_returns = returns[idx]\n",
    "                        mb_advantages = advantages[idx]\n",
    "                        \n",
    "                        # Get current log probs and values\n",
    "                        new_log_probs, entropy, values = model.evaluate_actions(mb_states, mb_actions)\n",
    "                        \n",
    "                        # Calculate ratio (π_θ / π_θ_old)\n",
    "                        ratio = torch.exp(new_log_probs - mb_old_log_probs)\n",
    "                        \n",
    "                        # PPO update\n",
    "                        surr1 = ratio * mb_advantages\n",
    "                        surr2 = torch.clamp(ratio, 1.0 - eps_clip, 1.0 + eps_clip) * mb_advantages\n",
    "                        \n",
    "                        # Calculate actor loss\n",
    "                        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                        \n",
    "                        # Calculate critic loss\n",
    "                        critic_loss = nn.MSELoss()(values, mb_returns)\n",
    "                        \n",
    "                        # Calculate total loss\n",
    "                        loss = actor_loss + value_coef * critic_loss - entropy_coef * entropy\n",
    "                        \n",
    "                        # Update network\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        # Optional: clip gradients for stability\n",
    "                        nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Clear buffer after update\n",
    "                buffer.clear()\n",
    "        \n",
    "        # End of episode processing\n",
    "        episodes_completed += 1\n",
    "        rewards_history.append(episode_reward)\n",
    "        \n",
    "        # Exponential moving average for smoother reporting\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward if episodes_completed > 1 else episode_reward\n",
    "        \n",
    "        if episodes_completed % 10 == 0:\n",
    "            print(f\"Episode {episodes_completed}, Reward: {episode_reward:.2f}, Avg Reward: {running_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return rewards_history\n",
    "\n",
    "# Helper function for PPO's Generalized Advantage Estimation\n",
    "def compute_gae(rewards, dones, values, next_states, model, gamma=0.99, lambda_gae=0.95):\n",
    "    returns = []\n",
    "    gae = 0\n",
    "    \n",
    "    # Move the model to evaluation mode for inference\n",
    "    with torch.no_grad():\n",
    "        # Get next state values for the last state\n",
    "        next_state = torch.FloatTensor(next_states[-1]).unsqueeze(0).to(device)\n",
    "        next_value = model.critic(next_state).cpu().numpy()[0, 0]\n",
    "    \n",
    "    # Append next value to values for easier computation\n",
    "    values = np.append(values, next_value)\n",
    "    \n",
    "    # Compute returns with GAE\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        if step == len(rewards) - 1:\n",
    "            next_non_terminal = 1.0 - dones[step]\n",
    "            next_return = next_value\n",
    "        else:\n",
    "            next_non_terminal = 1.0 - dones[step]\n",
    "            next_return = returns[0]\n",
    "        \n",
    "        # Calculate delta and GAE\n",
    "        delta = rewards[step] + gamma * values[step + 1] * next_non_terminal - values[step]\n",
    "        gae = delta + gamma * lambda_gae * next_non_terminal * gae\n",
    "        \n",
    "        # Insert at the beginning (as we're going backwards)\n",
    "        returns.insert(0, gae + values[step])\n",
    "    \n",
    "    return np.array(returns)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 5. Create environment functions for each uncertainty (unchanged)\n",
    "# ------------------------------------------------------\n",
    "\n",
    "def make_base_env():\n",
    "    # Base environment (no uncertainty)\n",
    "    env = gym.make('HalfCheetah-v4')\n",
    "    return env\n",
    "\n",
    "def make_sensor_noise_env():\n",
    "    env = gym.make('HalfCheetah-v4')\n",
    "    env = SensorNoiseWrapper(env, noise_std=0.01)\n",
    "    return env\n",
    "\n",
    "def make_motor_noise_env():\n",
    "    env = gym.make('HalfCheetah-v4')\n",
    "    env = MotorNoiseWrapper(env, noise_std=0.05)\n",
    "    return env\n",
    "\n",
    "def make_mass_variability_env():\n",
    "    env = gym.make('HalfCheetah-v4')\n",
    "    env = MassVariabilityWrapper(env, mass_variation_range=(0.8, 1.2))\n",
    "    return env\n",
    "\n",
    "def make_terrain_resistance_env():\n",
    "    env = gym.make('HalfCheetah-v4')\n",
    "    env = TerrainResistanceWrapper(env, drag_range=(0.0, 0.3))\n",
    "    return env\n",
    "\n",
    "def make_external_force_env():\n",
    "    env = gym.make('HalfCheetah-v4')\n",
    "    env = ExternalForceWrapper(env, force_magnitude_range=(-100.0, 100.0), pulse_probability=0.05)\n",
    "    return env\n",
    "\n",
    "def make_variable_contact_env():\n",
    "    env = gym.make('HalfCheetah-v4')\n",
    "    env = VariableContactWrapper(env, stiffness_range=(1.0, 10.0), damping_range=(0.1, 1.0))\n",
    "    return env\n",
    "\n",
    "def make_disruption_env():\n",
    "    env = gym.make('HalfCheetah-v4')\n",
    "    env = DisruptionWrapper(env, lambda_rate=0.1, intensity_scale=0.05)\n",
    "    return env\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. Run experiments and collect performance metrics\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Define the uncertainty experiments\n",
    "experiments = {\n",
    "    \"Base\": make_base_env,\n",
    "    \"Sensor Noise\": make_sensor_noise_env,\n",
    "    \"Motor Noise\": make_motor_noise_env,\n",
    "    \"Mass Variability\": make_mass_variability_env,\n",
    "    \"Terrain Resistance\": make_terrain_resistance_env,\n",
    "    \"External Force\": make_external_force_env, \n",
    "    \"Variable Contact\": make_variable_contact_env,\n",
    "    \"Disruption Model\": make_disruption_env\n",
    "}\n",
    "\n",
    "results = {}\n",
    "episodes = 300  # Adjust as needed for HalfCheetah\n",
    "\n",
    "for key, env_fn in experiments.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training with {key} uncertainty:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    rewards = train_ppo(env_fn, episodes=episodes)\n",
    "    results[key] = rewards\n",
    "    \n",
    "    # Save intermediate results after each experiment (in case of crash)\n",
    "    np.save(f\"rewards_ppo_{key.replace(' ', '_').lower()}.npy\", np.array(rewards))\n",
    "\n",
    "# --------------------------------------\n",
    "# 7. Plot the training performance curves with improved visualization\n",
    "# --------------------------------------\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Apply smoothing for clearer visualization\n",
    "window_size = 20\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(experiments)))\n",
    "\n",
    "for i, (key, rewards) in enumerate(results.items()):\n",
    "    # Compute rolling average for smoother curves\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    \n",
    "    # Plot both raw (light) and smoothed (dark) curves\n",
    "    plt.plot(rewards, alpha=0.3, color=colors[i])\n",
    "    plt.plot(range(window_size-1, len(rewards)), smoothed_rewards, \n",
    "             label=key, linewidth=2, color=colors[i])\n",
    "\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Reward\", fontsize=14)\n",
    "plt.title(\"PPO Performance with Different Aleatoric Uncertainties in HalfCheetah\", fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the final plot\n",
    "plt.savefig(\"halfcheetah_ppo_uncertainty_performance.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Also save all results together\n",
    "np.save(\"ppo_all_results.npy\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d2d9ac-8e6f-46a8-9682-ad90a5005d84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
